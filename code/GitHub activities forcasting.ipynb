{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "from statsmodels.tsa.exponential_smoothing.ets import ETSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = pd.read_csv('../data/activities.csv')\n",
    "activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 600 events for each contributor\n",
    "data = ( \n",
    "    activities\n",
    "    .groupby('contributor')\n",
    "    .tail(600)\n",
    "    .groupby('contributor')\n",
    "    .filter(lambda x: len(x) == 600)\n",
    ")\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic frequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_frequency(time_range):\n",
    "    if time_range >= 60 * 60 * 24:\n",
    "        return 'D'  # Daily if time range for one activity is a day or more\n",
    "    elif time_range >= 60 * 60:\n",
    "        time_range_h = time_range // 3600\n",
    "        freq = int(24 // ((24 // time_range_h) + 1)) if time_range_h > 1 else 1\n",
    "        return f\"{freq}H\"  # Hourly frequency\n",
    "    elif time_range >= 60:\n",
    "        time_range_m = time_range // 60\n",
    "        freq = int(60 // ((60 // time_range_m) + 1)) if time_range_m > 1 else 1\n",
    "        return f\"{freq}T\"  # Minutely frequency\n",
    "    else:\n",
    "        freq = int(60 // ((60 // time_range) + 1)) if time_range > 1 else 1\n",
    "        return f\"{freq}S\"  # Secondly frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_to_seasonality(freq):\n",
    "    unit = freq[-1]\n",
    "    if unit == 'D':\n",
    "        return 7\n",
    "    elif unit == 'H':\n",
    "        return 24 // int(freq[:-1])\n",
    "    elif unit == 'T':\n",
    "        return 24 * (60 // int(freq[:-1]))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((24*60*60)//((24*60*60)//(49*60*60)+1))//3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_name = data['contributor'].sample().values[0]\n",
    "random_user = data[data['contributor'] == random_name].reset_index(drop=True)\n",
    "\n",
    "result = (\n",
    "    random_user\n",
    "    .groupby([pd.Grouper(key='date', freq='D')])['activity']\n",
    "    .count()\n",
    "    .reset_index(name='n_activities')\n",
    "    .set_index('date')\n",
    "    .resample('D')\n",
    "    .sum()\n",
    "    .rename_axis(None)\n",
    ")\n",
    "result.plot(figsize=(14, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mean approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the frequency based on the mean of time differences between activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_based_frequency(contributor):\n",
    "    train_data = contributor.head(300)\n",
    "    train_data = train_data[train_data['date'] >= train_data['date'].max() - pd.DateOffset(months=3)]\n",
    "    time_range = (train_data['date'].iloc[-1] - train_data['date'].iloc[0]).total_seconds()//len(train_data)\n",
    "    print(\"Time range is :\", time_range)\n",
    "    frequency = map_frequency(time_range)\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "print(\"Frequence is :\", mean_based_frequency(random_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantile approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the frequency based on the median of time differences between activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_based_frequency(contributor, quantile=0.5):\n",
    "    train_data = contributor.head(300)\n",
    "    train_data = train_data[train_data['date'] >= train_data['date'].max() - pd.DateOffset(months=3)]\n",
    "    train_data['time_diff'] = train_data['date'].diff().dt.total_seconds()\n",
    "    time_range = train_data['time_diff'].quantile(quantile)\n",
    "    print(\"Time range is :\", time_range)\n",
    "    frequency = map_frequency(time_range)\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "print(\"Frequence is :\", quantile_based_frequency(random_user, quantile=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smallest time approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the frequency based on the smallest time to do a specified number of activities (= 100) by sliding over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smallest_time_based_frequency(contributor, window_size=100):\n",
    "    train_data = contributor.head(300)\n",
    "    train_data = train_data[train_data['date'] >= train_data['date'].max() - pd.DateOffset(months=3)]\n",
    "\n",
    "    smallest_time = None\n",
    "    # iterate over contributor's data in a sliding window\n",
    "    for i in range(len(train_data) - window_size + 1):\n",
    "        window = train_data.iloc[i:i + window_size]\n",
    "        time_span = window['date'].iloc[-1] - window['date'].iloc[0]\n",
    "\n",
    "        # the smallest time span\n",
    "        if smallest_time is None or time_span < smallest_time:\n",
    "            smallest_time = time_span\n",
    "\n",
    "    time_range = smallest_time.total_seconds()//window_size\n",
    "\n",
    "    print(\"Time range is :\", time_range)\n",
    "    frequency = map_frequency(time_range)\n",
    "\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "print(\"Frequence is :\", smallest_time_based_frequency(random_user, window_size=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_activities(contributor, frequency):\n",
    "\n",
    "    result = (\n",
    "        contributor\n",
    "        .groupby([pd.Grouper(key='date', freq=frequency)])['activity']\n",
    "        .count()\n",
    "        .reset_index(name='n_activities')\n",
    "        .set_index('date')\n",
    "        .resample(frequency)\n",
    "        .sum()\n",
    "        .rename_axis(None)\n",
    "        .assign(\n",
    "            cumsum_activities = lambda x: x['n_activities'].cumsum()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    train, test = (\n",
    "        result[result['cumsum_activities'] <= 300]\n",
    "        .apply(lambda x : x[x.index >= x.index.max() - pd.DateOffset(months=3)])\n",
    "        .drop('cumsum_activities', axis=1),\n",
    "\n",
    "        result[result['cumsum_activities'] > 300]\n",
    "        .drop('cumsum_activities', axis=1)\n",
    "    )\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = smallest_time_based_frequency(random_user, window_size=100)\n",
    "train, test = split_activities(random_user, frequency)\n",
    "train.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctd_score(y_true, y_pred, target_value):\n",
    "    true_cumsum = np.cumsum(y_true)\n",
    "    pred_cumsum = np.cumsum(y_pred)\n",
    "\n",
    "    if np.sum(y_pred) < target_value or np.sum(y_true) < target_value:\n",
    "        return None\n",
    "\n",
    "    time_true = np.argmax(true_cumsum >= target_value)\n",
    "    time_pred = np.argmax(pred_cumsum >= target_value)\n",
    "\n",
    "    return time_true - time_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_model(contributor):\n",
    "\n",
    "    contributor_name = contributor['contributor'].iloc[0]\n",
    "    contributor_category = contributor['category'].iloc[0]\n",
    "\n",
    "    frequency = mean_based_frequency(contributor)\n",
    "    seasonality = frequency_to_seasonality(frequency)\n",
    "\n",
    "    train, test = split_activities(contributor, frequency)\n",
    "\n",
    "    print(contributor_name, frequency, seasonality)\n",
    "\n",
    "    # choosing best parameters (possible seasonality)\n",
    "    if seasonality and (len(train)//seasonality) >= 2:\n",
    "        order = (1, 1, 1)\n",
    "        seasonal_order = (1, 1, 1, seasonality)\n",
    "    else:\n",
    "        order = (int((len(train)/3)), 1, 0)\n",
    "        seasonal_order = None\n",
    "        seasonality = None\n",
    "\n",
    "    model = SARIMAX(\n",
    "        train['n_activities'],\n",
    "        order = order,\n",
    "        seasonal_order = seasonal_order, \n",
    "        enforce_invertibility = True, \n",
    "        enforce_stationarity = False\n",
    "        ).fit(disp=False, method='lbfgs')\n",
    "\n",
    "    # Forecast the test set using confidence interval with 95%\n",
    "    predictions = model.get_prediction(start=len(train), end=len(train)+len(test)-1).summary_frame(alpha=0.05)\n",
    "\n",
    "    metrics = pd.Series({\n",
    "        'contributor': contributor_name,\n",
    "        'category': contributor_category,\n",
    "        'r2': r2_score(test['n_activities'], predictions['mean']),\n",
    "        'mae': mean_absolute_error(test['n_activities'], predictions['mean']),\n",
    "        'ctd_100': ctd_score(test['n_activities'], predictions['mean'], 100),\n",
    "        'ctd_200': ctd_score(test['n_activities'], predictions['mean'], 200),\n",
    "        'ctd_300': ctd_score(test['n_activities'], predictions['mean'], 300),\n",
    "        'n_activities': train['n_activities'].sum(),\n",
    "        'frequency': frequency,\n",
    "        'seasonality':seasonality,\n",
    "        'data_points':len(train),\n",
    "    })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_results = data.groupby(['category', 'contributor']).apply(sarima_model).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_results.to_csv('../models-evaluation/sarima_model-mean-v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unobserved components model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uc_model(contributor):\n",
    "\n",
    "    contributor_name = contributor['contributor'].iloc[0]\n",
    "    contributor_category = contributor['category'].iloc[0]\n",
    "\n",
    "    frequency = mean_based_frequency(contributor)\n",
    "    seasonality = frequency_to_seasonality(frequency)\n",
    "\n",
    "    train, test = split_activities(contributor, frequency)\n",
    "\n",
    "    print(contributor_name, frequency, seasonality)\n",
    "\n",
    "    # choosing best parameters (possible seasonality)\n",
    "    if (len(train)//seasonality) < 2:\n",
    "        seasonality = None\n",
    "\n",
    "    model = UnobservedComponents(\n",
    "        train['n_activities'], \n",
    "        level = True, \n",
    "        seasonal = seasonality\n",
    "        ).fit(disp=False, method='lbfgs')\n",
    "    \n",
    "    # Forecast the test set using confidence interval with 95%\n",
    "    predictions = model.get_prediction(start=len(train), end=len(train)+len(test)-1).summary_frame(alpha=0.05)\n",
    "\n",
    "    metrics = pd.Series({\n",
    "        'contributor': contributor_name,\n",
    "        'category': contributor_category,\n",
    "        'r2': r2_score(test['n_activities'], predictions['mean']),\n",
    "        'mae': mean_absolute_error(test['n_activities'], predictions['mean']),\n",
    "        'ctd_100': ctd_score(test['n_activities'], predictions['mean'], 100),\n",
    "        'ctd_200': ctd_score(test['n_activities'], predictions['mean'], 200),\n",
    "        'ctd_300': ctd_score(test['n_activities'], predictions['mean'], 300),\n",
    "        'n_activities': train['n_activities'].sum(),\n",
    "        'frequency': frequency,\n",
    "        'seasonality':seasonality,\n",
    "        'data_points':len(train),\n",
    "    })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_results = data.groupby(['category', 'contributor']).apply(uc_model).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_results.to_csv('../models-evaluation/uc_model-mean-v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Trend and Seasonality model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ets_model(contributor):\n",
    "\n",
    "    contributor_name = contributor['contributor'].iloc[0]\n",
    "    contributor_category = contributor['category'].iloc[0]\n",
    "\n",
    "    frequency = mean_based_frequency(contributor)\n",
    "    seasonality = frequency_to_seasonality(frequency)\n",
    "\n",
    "    train, test = split_activities(contributor, frequency)\n",
    "\n",
    "    print(contributor_name, frequency, seasonality)\n",
    "\n",
    "    # choosing best parameters (possible seasonality)\n",
    "    if seasonality and (len(train)//seasonality) >= 2:\n",
    "        seasonal = 'add'\n",
    "    else:\n",
    "        seasonality = None\n",
    "        seasonal = None\n",
    "\n",
    "    model = ETSModel(\n",
    "        train['n_activities'], \n",
    "        error = 'add', \n",
    "        trend = 'add', \n",
    "        seasonal = seasonal, \n",
    "        seasonal_periods = seasonality\n",
    "    ).fit(disp=False)\n",
    "\n",
    "    # Forecast the test set using confidence interval with 95%\n",
    "    predictions = model.get_prediction(start=len(train), end=len(train)+len(test)-1).summary_frame(alpha=0.05)\n",
    "\n",
    "    metrics = pd.Series({\n",
    "        'contributor': contributor_name,\n",
    "        'category': contributor_category,\n",
    "        'r2': r2_score(test['n_activities'], predictions['mean']),\n",
    "        'mae': mean_absolute_error(test['n_activities'], predictions['mean']),\n",
    "        'ctd_100': ctd_score(test['n_activities'], predictions['mean'], 100),\n",
    "        'ctd_200': ctd_score(test['n_activities'], predictions['mean'], 200),\n",
    "        'ctd_300': ctd_score(test['n_activities'], predictions['mean'], 300),\n",
    "        'n_activities': train['n_activities'].sum(),\n",
    "        'frequency': frequency,\n",
    "        'seasonality':seasonality,\n",
    "        'data_points':len(train),\n",
    "    })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ets_results = data.groupby(['category', 'contributor']).apply(ets_model).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ets_results.to_csv('../models-evaluation/ets_model-mean-v1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
